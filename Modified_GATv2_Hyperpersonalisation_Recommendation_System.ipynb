{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Customer ID  Customer Type       Industry  Company Size Location  \\\n",
            "0     CUS-067  Institutional  Manufacturing          6828   London   \n",
            "1     CUS-685     Government     Technology          8392      USA   \n",
            "2     CUS-999     Government  Manufacturing          2297   London   \n",
            "3     CUS-504      Corporate  Manufacturing          3287      USA   \n",
            "4     CUS-004     Government  Manufacturing           391     Asia   \n",
            "\n",
            "  Relationship Manager Product ID     Product Type Product Category  \\\n",
            "0               RM-001   PROD-772             Loan  Cash Management   \n",
            "1               RM-003   PROD-772  Risk Management   Trade Services   \n",
            "2               RM-003   PROD-465          Deposit  Cash Management   \n",
            "3           John Smith   PROD-149  Risk Management          Lending   \n",
            "4               RM-003   PROD-622             Loan   Trade Services   \n",
            "\n",
            "    Product Subcategory  ... Product Adoption Product Usage  \\\n",
            "0      Foreign Exchange  ...                2             3   \n",
            "1      Foreign Exchange  ...                3             2   \n",
            "2  Supply Chain Finance  ...                4             4   \n",
            "3            Term Loans  ...                5             7   \n",
            "4      Commercial Cards  ...                1             2   \n",
            "\n",
            "  Customer Engagement Credit Score  Risk Rating Default Probability  \\\n",
            "0                   4   647.961444       Medium            0.049433   \n",
            "1                   9   665.868526          Low            0.050766   \n",
            "2                   6   642.795297       Medium            0.057444   \n",
            "3                   9   642.730190         High            0.020513   \n",
            "4                   2   746.712414          Low            0.062103   \n",
            "\n",
            "       Exposure  Market Data  Economic Indicators  Regulatory Requirements  \n",
            "0  32823.402317   908.079721             3.918120                Basel III  \n",
            "1  46560.687222   824.154844             2.231064                        s  \n",
            "2  38027.519698   316.309772             3.756642                        s  \n",
            "3  18288.511309   348.140325             2.289096                        s  \n",
            "4  43672.493310   636.374182             2.917378                        s  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the possible values for each feature\n",
        "customer_types = [\"Corporate\", \"Institutional\", \"Government\"]\n",
        "industries = [\"Finance\", \"Healthcare\", \"Technology\", \"Energy\", \"Manufacturing\"]\n",
        "locations = [\"USA\", \"Europe\", \"Asia\", \"New York\", \"London\", \"Tokyo\"]\n",
        "relationship_managers = [\"John Smith\", \"Jane Doe\", \"RM-001\", \"RM-002\", \"RM-003\"]\n",
        "\n",
        "product_types = [\"Loan\", \"Deposit\", \"Trade Finance\", \"Cash Management\", \"Risk Management\"]\n",
        "product_categories = [\"Cash Management\", \"Risk Management\", \"Trade Services\", \"Lending\", \"Investments\"]\n",
        "product_subcategories = [\"Credit Facilities\", \"Foreign Exchange\", \"Supply Chain Finance\", \"Term Loans\", \"Commercial Cards\"]\n",
        "product_features = [\"Interest Rate\", \"Tenor\", \"Currency\", \"Collateral\", \"Covenants\"]\n",
        "\n",
        "transaction_types = [\"Loan Disbursement\", \"Deposit\", \"Trade Settlement\", \"Credit Facility\", \"FX Trade\"]\n",
        "currencies = [\"USD\", \"EUR\", \"GBP\", \"JPY\", \"CHF\"]\n",
        "\n",
        "risk_ratings = [\"Low\", \"Medium\", \"High\"]\n",
        "regulatory_requirements = [\"KYC\", \"AML\", \"CCAR\", \"Basel III\", \"Dodd-Frank\"]\n",
        "\n",
        "# Define the synthetic data generation rules\n",
        "def generate_customer_id():\n",
        "    return f\"CUS-{random.randint(1, 1000):03d}\"\n",
        "\n",
        "def generate_customer_type():\n",
        "    return random.choice(customer_types)\n",
        "\n",
        "def generate_industry():\n",
        "    return random.choice(industries)\n",
        "\n",
        "def generate_company_size():\n",
        "    return random.randint(100, 10000)\n",
        "\n",
        "def generate_location():\n",
        "    return random.choice(locations)\n",
        "\n",
        "def generate_relationship_manager():\n",
        "    return random.choice(relationship_managers)\n",
        "\n",
        "def generate_product_id():\n",
        "    return f\"PROD-{random.randint(1, 1000):03d}\"\n",
        "\n",
        "def generate_product_type():\n",
        "    return random.choice(product_types)\n",
        "\n",
        "def generate_product_category():\n",
        "    return random.choice(product_categories)\n",
        "\n",
        "def generate_product_subcategory():\n",
        "    return random.choice(product_subcategories)\n",
        "\n",
        "def generate_product_features():\n",
        "    return random.choice(product_features)\n",
        "\n",
        "def generate_transaction_id():\n",
        "    return f\"TRAN-{random.randint(1, 1000):03d}\"\n",
        "\n",
        "def generate_transaction_date():\n",
        "    return datetime.now() - timedelta(days=random.randint(1, 365))\n",
        "\n",
        "def generate_transaction_type():\n",
        "    return random.choice(transaction_types)\n",
        "\n",
        "def generate_transaction_amount():\n",
        "    return random.uniform(1000.0, 100000.0)\n",
        "\n",
        "def generate_transaction_currency():\n",
        "    return random.choice(currencies)\n",
        "\n",
        "def generate_transaction_frequency():\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "def generate_transaction_value():\n",
        "    return random.uniform(10000.0, 100000.0)\n",
        "\n",
        "def generate_product_adoption():\n",
        "    return random.randint(1, 5)\n",
        "\n",
        "def generate_product_usage():\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "def generate_customer_engagement():\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "def generate_credit_score():\n",
        "    return random.uniform(600.0, 800.0)\n",
        "\n",
        "def generate_risk_rating():\n",
        "    return random.choice(risk_ratings)\n",
        "\n",
        "def generate_default_probability():\n",
        "    return random.uniform(0.01, 0.10)\n",
        "\n",
        "def generate_exposure():\n",
        "    return random.uniform(10000.0, 100000.0)\n",
        "\n",
        "def generate_market_data():\n",
        "    return random.uniform(100.0, 1000.0)\n",
        "\n",
        "def generate_economic_indicators():\n",
        "    return random.uniform(2.0, 4.0)\n",
        "\n",
        "def generate_regulatory_requirements():\n",
        "    return random.choice(regulatory_requirements)\n",
        "\n",
        "# Generate the synthetic data\n",
        "data = []\n",
        "for i in range(1000):\n",
        "    customer_id = generate_customer_id()\n",
        "    customer_type = generate_customer_type()\n",
        "    industry = generate_industry()\n",
        "    company_size = generate_company_size()\n",
        "    location = generate_location()\n",
        "    relationship_manager = generate_relationship_manager()\n",
        "    \n",
        "    product_id = generate_product_id()\n",
        "    product_type = generate_product_type()\n",
        "    product_category = generate_product_category()\n",
        "    product_subcategory = generate_product_subcategory()\n",
        "    product_features = generate_product_features()\n",
        "    \n",
        "    transaction_id = generate_transaction_id()\n",
        "    transaction_date = generate_transaction_date()\n",
        "    transaction_type = generate_transaction_type()\n",
        "    transaction_amount = generate_transaction_amount()\n",
        "    transaction_currency = generate_transaction_currency()\n",
        "    \n",
        "    transaction_frequency = generate_transaction_frequency()\n",
        "    transaction_value = generate_transaction_value()\n",
        "    product_adoption = generate_product_adoption()\n",
        "    product_usage = generate_product_usage()\n",
        "    customer_engagement = generate_customer_engagement()\n",
        "    \n",
        "    credit_score = generate_credit_score()\n",
        "    risk_rating = generate_risk_rating()\n",
        "    default_probability = generate_default_probability()\n",
        "    exposure = generate_exposure()\n",
        "    \n",
        "    market_data = generate_market_data()\n",
        "    economic_indicators = generate_economic_indicators()\n",
        "    regulatory_requirements = generate_regulatory_requirements()\n",
        "    \n",
        "    data.append({\n",
        "        \"Customer ID\": customer_id,\n",
        "        \"Customer Type\": customer_type,\n",
        "        \"Industry\": industry,\n",
        "        \"Company Size\": company_size,\n",
        "        \"Location\": location,\n",
        "        \"Relationship Manager\": relationship_manager,\n",
        "        \n",
        "        \"Product ID\": product_id,\n",
        "        \"Product Type\": product_type,\n",
        "        \"Product Category\": product_category,\n",
        "        \"Product Subcategory\": product_subcategory,\n",
        "        \"Product Features\": product_features,\n",
        "        \n",
        "        \"Transaction ID\": transaction_id,\n",
        "        \"Transaction Date\": transaction_date,\n",
        "        \"Transaction Type\": transaction_type,\n",
        "        \"Transaction Amount\": transaction_amount,\n",
        "        \"Transaction Currency\": transaction_currency,\n",
        "        \n",
        "        \"Transaction Frequency\": transaction_frequency,\n",
        "        \"Transaction Value\": transaction_value,\n",
        "        \"Product Adoption\": product_adoption,\n",
        "        \"Product Usage\": product_usage,\n",
        "        \"Customer Engagement\": customer_engagement,\n",
        "        \n",
        "        \"Credit Score\": credit_score,\n",
        "        \"Risk Rating\": risk_rating,\n",
        "        \"Default Probability\": default_probability,\n",
        "        \"Exposure\": exposure,\n",
        "        \n",
        "        \"Market Data\": market_data,\n",
        "        \"Economic Indicators\": economic_indicators,\n",
        "        \"Regulatory Requirements\": regulatory_requirements\n",
        "    })\n",
        "\n",
        "# Create a Pandas DataFrame from the synthetic data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the synthetic data to a CSV file\n",
        "df.to_csv(\"wholesale_banking_synthetic_data.csv\", index=False)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Found indices in 'edge_index' that are larger than 0 (got 2707). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 1) in your node feature matrix and try again.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:317\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: INDICES element is out of DATA bounds, id=633 axis_dim=1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m201\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     train_acc, val_acc, test_acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[3], line 107\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    106\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 107\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m    109\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 78\u001b[0m, in \u001b[0;36mGATv2Model.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 78\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     80\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     38\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch_geometric\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39msoftmax(alpha, edge_index[\u001b[38;5;241m0\u001b[39m], num_nodes\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     39\u001b[0m alpha \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(alpha, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:545\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[0;32m    543\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[1;32m--> 545\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mcollect_param_data(\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:407\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m    406\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, dim, data)\n\u001b[1;32m--> 407\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:367\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    365\u001b[0m         index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[0;32m    370\u001b[0m     row, col, _ \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mcoo()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:313\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_select_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:328\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that are larger \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[1;31mIndexError\u001b[0m: Found indices in 'edge_index' that are larger than 0 (got 2707). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 1) in your node feature matrix and try again."
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from torch_geometric.nn import MessagePassing\n",
        "# from torch_geometric.datasets import Planetoid\n",
        "# import torch_geometric.utils\n",
        "# from torch_geometric.data import DataLoader\n",
        "# from sklearn.cluster import KMeans\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Define the GATv2 Layer\n",
        "# class GATv2Conv(MessagePassing):\n",
        "#     def __init__(self, in_channels, out_channels, heads=1, concat=True, negative_slope=0.2, dropout=0, bias=True, **kwargs):\n",
        "#         super(GATv2Conv, self).__init__(aggr='add', **kwargs)  # \"Add\" aggregation.\n",
        "#         self.heads = heads\n",
        "#         self.out_channels = out_channels\n",
        "#         self.lin = torch.nn.Linear(in_channels, out_channels * heads, bias=False)\n",
        "#         self.att = torch.nn.Parameter(torch.Tensor(1, heads, 2 * out_channels))\n",
        "#         self.negative_slope = negative_slope\n",
        "#         self.dropout = dropout\n",
        "#         self.concat = concat\n",
        "\n",
        "#         if bias:\n",
        "#             self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n",
        "#         else:\n",
        "#             self.register_parameter('bias', None)\n",
        "\n",
        "#         self.reset_parameters()\n",
        "\n",
        "#     def reset_parameters(self):\n",
        "#         torch.nn.init.xavier_uniform_(self.lin.weight)\n",
        "#         torch.nn.init.xavier_uniform_(self.att)\n",
        "#         if self.bias is not None:\n",
        "#             torch.nn.init.zeros_(self.bias)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         x = self.lin(x).view(-1, self.heads, self.out_channels)\n",
        "#         alpha = self._prepare_attentional_mechanism_input(x, edge_index)\n",
        "#         alpha = torch_geometric.utils.softmax(alpha, edge_index[0], num_nodes=x.size(0))\n",
        "#         alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "#         return self.propagate(edge_index, x=x, alpha=alpha)\n",
        "\n",
        "#     def _prepare_attentional_mechanism_input(self, x, edge_index):\n",
        "#         x_i = x[edge_index[0]]\n",
        "#         x_j = x[edge_index[1]]\n",
        "#         alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "#         alpha = F.leaky_relu((alpha * self.att).sum(dim=-1), self.negative_slope)\n",
        "#         return alpha\n",
        "\n",
        "#     def message(self, x_j, alpha):\n",
        "#         return alpha.unsqueeze(-1) * x_j\n",
        "\n",
        "#     def aggregate(self, inputs, index, dim_size=None):\n",
        "#         return torch_geometric.utils.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=self.aggr)\n",
        "\n",
        "#     def update(self, aggr_out):\n",
        "#         if self.concat:\n",
        "#             aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
        "#         else:\n",
        "#             aggr_out = aggr_out.mean(dim=1)\n",
        "\n",
        "#         if self.bias is not None:\n",
        "#             aggr_out = aggr_out + self.bias\n",
        "\n",
        "#         return aggr_out\n",
        "\n",
        "# # Define the GATv2 Model\n",
        "# class GATv2Model(torch.nn.Module):\n",
        "#     def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
        "#         super(GATv2Model, self).__init__()\n",
        "#         self.convs = torch.nn.ModuleList()\n",
        "#         self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=1))\n",
        "#         for _ in range(num_layers - 2):\n",
        "#             self.convs.append(GATv2Conv(hidden_channels, hidden_channels, heads=1))\n",
        "#         self.convs.append(GATv2Conv(hidden_channels, out_channels, heads=1))\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         for conv in self.convs[:-1]:\n",
        "#             x = conv(x, edge_index)\n",
        "#             x = F.relu(x)\n",
        "#             x = F.dropout(x, p=0.6, training=self.training)\n",
        "#         x = self.convs[-1](x, edge_index)\n",
        "#         return x\n",
        "\n",
        "# # Load data\n",
        "# dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "# data = dataset[0]\n",
        "\n",
        "# # Ensure all indices in edge_index are within the valid range\n",
        "# num_nodes = data.x.size(0)\n",
        "# mask = (data.edge_index[0] < num_nodes) & (data.edge_index[1] < num_nodes)\n",
        "# data.edge_index = data.edge_index[:, mask]\n",
        "\n",
        "# # Check for any invalid edge indices after filtering\n",
        "# if data.edge_index.max() >= num_nodes:\n",
        "#     raise ValueError(\"There are still invalid edge indices in 'edge_index'.\")\n",
        "\n",
        "# # Initialize the model\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = GATv2Model(dataset.num_features, 8, dataset.num_classes, num_layers=3).to(device)\n",
        "# data = data.to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "# # Training function\n",
        "# def train():\n",
        "#     model.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model(data.x, data.edge_index)\n",
        "#     loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     return loss.item()\n",
        "\n",
        "# # Testing function\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     logits, accs = model(data.x, data.edge_index), []\n",
        "#     for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "#         pred = logits[mask].max(1)[1]\n",
        "#         acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "#         accs.append(acc)\n",
        "#     return accs\n",
        "\n",
        "# # Train the model\n",
        "# for epoch in range(1, 201):\n",
        "#     loss = train()\n",
        "#     train_acc, val_acc, test_acc = test()\n",
        "#     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
        "\n",
        "# # Get embeddings from the trained model\n",
        "# model.eval()\n",
        "# embeddings = model(data.x, data.edge_index).detach().cpu().numpy()\n",
        "\n",
        "# # Apply K-means clustering\n",
        "# n_clusters = 5\n",
        "# kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)\n",
        "# clusters = kmeans.labels_\n",
        "\n",
        "# # Map clusters back to customer IDs (assuming customer_ids is available)\n",
        "# customer_ids = range(len(embeddings))  # Replace this with actual customer IDs\n",
        "# customer_cluster_map = {customer_ids[i]: clusters[i] for i in range(len(customer_ids))}\n",
        "\n",
        "# # Display customer segments\n",
        "# for cluster_id in range(n_clusters):\n",
        "#     print(f\"Customers in segment {cluster_id}:\")\n",
        "#     members = [cid for cid, clust in customer_cluster_map.items() if clust == cluster_id]\n",
        "#     print(members)\n",
        "\n",
        "# # Optional: Visualize the clusters\n",
        "# plt.figure(figsize=(10, 7))\n",
        "# for cluster_id in range(n_clusters):\n",
        "#     cluster_points = embeddings[clusters == cluster_id]\n",
        "#     plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_id}')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.6179, Train: 0.4000, Val: 0.4750, Test: 0.4050\n",
            "Epoch: 002, Loss: 1.5712, Train: 0.4611, Val: 0.5375, Test: 0.4450\n",
            "Epoch: 003, Loss: 1.4549, Train: 0.5250, Val: 0.5875, Test: 0.4950\n",
            "Epoch: 004, Loss: 1.4775, Train: 0.5625, Val: 0.5625, Test: 0.4950\n",
            "Epoch: 005, Loss: 1.2778, Train: 0.6014, Val: 0.5625, Test: 0.4950\n",
            "Epoch: 006, Loss: 1.2775, Train: 0.6431, Val: 0.5750, Test: 0.4950\n",
            "Epoch: 007, Loss: 1.2551, Train: 0.6625, Val: 0.6000, Test: 0.6150\n",
            "Epoch: 008, Loss: 1.1929, Train: 0.6861, Val: 0.6000, Test: 0.6150\n",
            "Epoch: 009, Loss: 1.2008, Train: 0.6972, Val: 0.6125, Test: 0.6300\n",
            "Epoch: 010, Loss: 1.1025, Train: 0.7014, Val: 0.6000, Test: 0.6300\n",
            "Epoch: 011, Loss: 1.1623, Train: 0.7014, Val: 0.6375, Test: 0.6650\n",
            "Epoch: 012, Loss: 1.1160, Train: 0.7139, Val: 0.6375, Test: 0.6650\n",
            "Epoch: 013, Loss: 1.0759, Train: 0.7181, Val: 0.6375, Test: 0.6650\n",
            "Epoch: 014, Loss: 1.1087, Train: 0.7222, Val: 0.6500, Test: 0.6900\n",
            "Epoch: 015, Loss: 1.0430, Train: 0.7292, Val: 0.6375, Test: 0.6900\n",
            "Epoch: 016, Loss: 1.0459, Train: 0.7292, Val: 0.6375, Test: 0.6900\n",
            "Epoch: 017, Loss: 1.0368, Train: 0.7375, Val: 0.6500, Test: 0.6900\n",
            "Epoch: 018, Loss: 1.0661, Train: 0.7389, Val: 0.6625, Test: 0.6850\n",
            "Epoch: 019, Loss: 1.0325, Train: 0.7472, Val: 0.6875, Test: 0.6950\n",
            "Epoch: 020, Loss: 1.0391, Train: 0.7458, Val: 0.7125, Test: 0.7050\n",
            "Epoch: 021, Loss: 1.0098, Train: 0.7458, Val: 0.7125, Test: 0.7050\n",
            "Epoch: 022, Loss: 1.0221, Train: 0.7514, Val: 0.7500, Test: 0.7100\n",
            "Epoch: 023, Loss: 1.0326, Train: 0.7583, Val: 0.7625, Test: 0.7150\n",
            "Epoch: 024, Loss: 0.9595, Train: 0.7694, Val: 0.7750, Test: 0.7050\n",
            "Epoch: 025, Loss: 1.0270, Train: 0.7764, Val: 0.7750, Test: 0.7050\n",
            "Epoch: 026, Loss: 0.9947, Train: 0.7792, Val: 0.7750, Test: 0.7050\n",
            "Epoch: 027, Loss: 1.0412, Train: 0.7778, Val: 0.8000, Test: 0.7250\n",
            "Epoch: 028, Loss: 0.9608, Train: 0.7806, Val: 0.8125, Test: 0.7300\n",
            "Epoch: 029, Loss: 0.9723, Train: 0.7792, Val: 0.8125, Test: 0.7300\n",
            "Epoch: 030, Loss: 0.9487, Train: 0.7778, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 031, Loss: 0.9469, Train: 0.7806, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 032, Loss: 0.9470, Train: 0.7833, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 033, Loss: 0.9918, Train: 0.7819, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 034, Loss: 0.9596, Train: 0.7819, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 035, Loss: 0.9705, Train: 0.7764, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 036, Loss: 1.0045, Train: 0.7806, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 037, Loss: 0.9619, Train: 0.7819, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 038, Loss: 0.9558, Train: 0.7806, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 039, Loss: 0.8884, Train: 0.7806, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 040, Loss: 0.9276, Train: 0.7833, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 041, Loss: 0.9217, Train: 0.7833, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 042, Loss: 0.9705, Train: 0.7847, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 043, Loss: 0.9053, Train: 0.7847, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 044, Loss: 0.9465, Train: 0.7903, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 045, Loss: 0.9202, Train: 0.7889, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 046, Loss: 0.8978, Train: 0.7875, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 047, Loss: 0.9096, Train: 0.7875, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 048, Loss: 0.8656, Train: 0.7931, Val: 0.8125, Test: 0.7350\n",
            "Epoch: 049, Loss: 0.9243, Train: 0.7944, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 050, Loss: 0.9108, Train: 0.8000, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 051, Loss: 0.8991, Train: 0.8014, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 052, Loss: 0.9369, Train: 0.8028, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 053, Loss: 0.9171, Train: 0.8083, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 054, Loss: 0.8822, Train: 0.8083, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 055, Loss: 0.9040, Train: 0.8111, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 056, Loss: 0.9381, Train: 0.8139, Val: 0.8250, Test: 0.7350\n",
            "Epoch: 057, Loss: 0.9174, Train: 0.8111, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 058, Loss: 0.9222, Train: 0.8139, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 059, Loss: 0.9346, Train: 0.8139, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 060, Loss: 0.9069, Train: 0.8153, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 061, Loss: 0.8568, Train: 0.8181, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 062, Loss: 0.8942, Train: 0.8236, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 063, Loss: 0.8924, Train: 0.8222, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 064, Loss: 0.8899, Train: 0.8236, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 065, Loss: 0.8689, Train: 0.8250, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 066, Loss: 0.8880, Train: 0.8250, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 067, Loss: 0.8931, Train: 0.8236, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 068, Loss: 0.8999, Train: 0.8236, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 069, Loss: 0.8802, Train: 0.8264, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 070, Loss: 0.8959, Train: 0.8236, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 071, Loss: 0.8792, Train: 0.8264, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 072, Loss: 0.8522, Train: 0.8236, Val: 0.8250, Test: 0.7600\n",
            "Epoch: 073, Loss: 0.8358, Train: 0.8319, Val: 0.8375, Test: 0.7600\n",
            "Epoch: 074, Loss: 0.9048, Train: 0.8347, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 075, Loss: 0.8824, Train: 0.8319, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 076, Loss: 0.8711, Train: 0.8333, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 077, Loss: 0.8574, Train: 0.8403, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 078, Loss: 0.8481, Train: 0.8431, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 079, Loss: 0.8832, Train: 0.8458, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 080, Loss: 0.8373, Train: 0.8458, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 081, Loss: 0.8630, Train: 0.8458, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 082, Loss: 0.8762, Train: 0.8500, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 083, Loss: 0.8894, Train: 0.8514, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 084, Loss: 0.8474, Train: 0.8528, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 085, Loss: 0.8599, Train: 0.8542, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 086, Loss: 0.8408, Train: 0.8514, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 087, Loss: 0.8609, Train: 0.8542, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 088, Loss: 0.8362, Train: 0.8569, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 089, Loss: 0.8540, Train: 0.8597, Val: 0.8500, Test: 0.7850\n",
            "Epoch: 090, Loss: 0.8853, Train: 0.8583, Val: 0.8625, Test: 0.8150\n",
            "Epoch: 091, Loss: 0.8670, Train: 0.8583, Val: 0.8625, Test: 0.8150\n",
            "Epoch: 092, Loss: 0.8936, Train: 0.8597, Val: 0.8625, Test: 0.8150\n",
            "Epoch: 093, Loss: 0.8113, Train: 0.8597, Val: 0.8625, Test: 0.8150\n",
            "Epoch: 094, Loss: 0.8200, Train: 0.8625, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 095, Loss: 0.8653, Train: 0.8569, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 096, Loss: 0.8259, Train: 0.8569, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 097, Loss: 0.8402, Train: 0.8569, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 098, Loss: 0.8617, Train: 0.8597, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 099, Loss: 0.8442, Train: 0.8611, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 100, Loss: 0.8282, Train: 0.8611, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 101, Loss: 0.8314, Train: 0.8639, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 102, Loss: 0.7936, Train: 0.8681, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 103, Loss: 0.8101, Train: 0.8722, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 104, Loss: 0.8257, Train: 0.8750, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 105, Loss: 0.8226, Train: 0.8792, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 106, Loss: 0.8204, Train: 0.8833, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 107, Loss: 0.8312, Train: 0.8833, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 108, Loss: 0.8638, Train: 0.8861, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 109, Loss: 0.8241, Train: 0.8847, Val: 0.8750, Test: 0.8150\n",
            "Epoch: 110, Loss: 0.7748, Train: 0.8889, Val: 0.8875, Test: 0.8750\n",
            "Epoch: 111, Loss: 0.7788, Train: 0.8917, Val: 0.8875, Test: 0.8750\n",
            "Epoch: 112, Loss: 0.7922, Train: 0.8944, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 113, Loss: 0.8265, Train: 0.8972, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 114, Loss: 0.7942, Train: 0.9000, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 115, Loss: 0.8183, Train: 0.9028, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 116, Loss: 0.7792, Train: 0.9028, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 117, Loss: 0.7863, Train: 0.9042, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 118, Loss: 0.8144, Train: 0.9111, Val: 0.8750, Test: 0.8750\n",
            "Epoch: 119, Loss: 0.7928, Train: 0.9153, Val: 0.8875, Test: 0.8750\n",
            "Epoch: 120, Loss: 0.7771, Train: 0.9208, Val: 0.8875, Test: 0.8750\n",
            "Epoch: 121, Loss: 0.7870, Train: 0.9278, Val: 0.8875, Test: 0.8750\n",
            "Epoch: 122, Loss: 0.7570, Train: 0.9292, Val: 0.8875, Test: 0.8750\n",
            "Epoch: 123, Loss: 0.8013, Train: 0.9319, Val: 0.9000, Test: 0.8950\n",
            "Epoch: 124, Loss: 0.8025, Train: 0.9333, Val: 0.9000, Test: 0.8950\n",
            "Epoch: 125, Loss: 0.7846, Train: 0.9375, Val: 0.9000, Test: 0.8950\n",
            "Epoch: 126, Loss: 0.7701, Train: 0.9444, Val: 0.9000, Test: 0.8950\n",
            "Epoch: 127, Loss: 0.7513, Train: 0.9486, Val: 0.9000, Test: 0.8950\n",
            "Epoch: 128, Loss: 0.7589, Train: 0.9500, Val: 0.9125, Test: 0.9200\n",
            "Epoch: 129, Loss: 0.7542, Train: 0.9528, Val: 0.9125, Test: 0.9200\n",
            "Epoch: 130, Loss: 0.7650, Train: 0.9542, Val: 0.9125, Test: 0.9200\n",
            "Epoch: 131, Loss: 0.7712, Train: 0.9583, Val: 0.9250, Test: 0.9200\n",
            "Epoch: 132, Loss: 0.7773, Train: 0.9597, Val: 0.9375, Test: 0.9250\n",
            "Epoch: 133, Loss: 0.7491, Train: 0.9625, Val: 0.9375, Test: 0.9250\n",
            "Epoch: 134, Loss: 0.7474, Train: 0.9667, Val: 0.9375, Test: 0.9250\n",
            "Epoch: 135, Loss: 0.7530, Train: 0.9681, Val: 0.9500, Test: 0.9400\n",
            "Epoch: 136, Loss: 0.7822, Train: 0.9694, Val: 0.9500, Test: 0.9400\n",
            "Epoch: 137, Loss: 0.7294, Train: 0.9722, Val: 0.9500, Test: 0.9400\n",
            "Epoch: 138, Loss: 0.7117, Train: 0.9736, Val: 0.9500, Test: 0.9400\n",
            "Epoch: 139, Loss: 0.7483, Train: 0.9736, Val: 0.9500, Test: 0.9400\n",
            "Epoch: 140, Loss: 0.7659, Train: 0.9792, Val: 0.9500, Test: 0.9400\n",
            "Epoch: 141, Loss: 0.7493, Train: 0.9778, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 142, Loss: 0.7304, Train: 0.9778, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 143, Loss: 0.7310, Train: 0.9819, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 144, Loss: 0.7245, Train: 0.9806, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 145, Loss: 0.7047, Train: 0.9819, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 146, Loss: 0.7176, Train: 0.9833, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 147, Loss: 0.7260, Train: 0.9861, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 148, Loss: 0.7322, Train: 0.9861, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 149, Loss: 0.7353, Train: 0.9861, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 150, Loss: 0.7253, Train: 0.9861, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 151, Loss: 0.7265, Train: 0.9875, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 152, Loss: 0.7232, Train: 0.9917, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 153, Loss: 0.6722, Train: 0.9931, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 154, Loss: 0.7270, Train: 0.9931, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 155, Loss: 0.6968, Train: 0.9944, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 156, Loss: 0.7311, Train: 0.9944, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 157, Loss: 0.6558, Train: 0.9944, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 158, Loss: 0.7065, Train: 0.9958, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 159, Loss: 0.6828, Train: 0.9944, Val: 0.9625, Test: 0.9600\n",
            "Epoch: 160, Loss: 0.6965, Train: 0.9944, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 161, Loss: 0.7122, Train: 0.9958, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 162, Loss: 0.6937, Train: 0.9958, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 163, Loss: 0.6705, Train: 0.9958, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 164, Loss: 0.6533, Train: 0.9944, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 165, Loss: 0.7027, Train: 0.9944, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 166, Loss: 0.7193, Train: 0.9944, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 167, Loss: 0.6645, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 168, Loss: 0.7022, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 169, Loss: 0.7091, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 170, Loss: 0.6921, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 171, Loss: 0.6520, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 172, Loss: 0.6688, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 173, Loss: 0.6819, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 174, Loss: 0.7009, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 175, Loss: 0.6826, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 176, Loss: 0.6956, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 177, Loss: 0.7049, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 178, Loss: 0.6828, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 179, Loss: 0.6376, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 180, Loss: 0.6942, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 181, Loss: 0.6720, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 182, Loss: 0.6460, Train: 0.9986, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 183, Loss: 0.6941, Train: 1.0000, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 184, Loss: 0.7026, Train: 1.0000, Val: 0.9750, Test: 0.9800\n",
            "Epoch: 185, Loss: 0.6782, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 186, Loss: 0.6647, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 187, Loss: 0.6916, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 188, Loss: 0.6613, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 189, Loss: 0.7356, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 190, Loss: 0.7078, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 191, Loss: 0.6736, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 192, Loss: 0.6710, Train: 1.0000, Val: 0.9875, Test: 0.9900\n",
            "Epoch: 193, Loss: 0.6825, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 194, Loss: 0.6647, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 195, Loss: 0.6818, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 196, Loss: 0.6817, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 197, Loss: 0.6887, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 198, Loss: 0.6348, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 199, Loss: 0.6450, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Epoch: 200, Loss: 0.7033, Train: 1.0000, Val: 0.9750, Test: 0.9900\n",
            "Final Test Accuracy: 0.9900\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "\n",
        "# Load the synthetic dataset\n",
        "df = pd.read_csv(\"wholesale_banking_synthetic_data.csv\")\n",
        "\n",
        "# Initialize LabelEncoders for categorical features\n",
        "label_encoders = {}\n",
        "for column in ['Customer Type', 'Industry', 'Location', 'Relationship Manager', 'Product Type', 'Product Category', 'Product Subcategory', 'Product Features', 'Transaction Type', 'Transaction Currency', 'Risk Rating', 'Regulatory Requirements']:\n",
        "    le = LabelEncoder()\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df.drop(columns=['Customer ID', 'Product ID', 'Transaction ID', 'Transaction Date']))\n",
        "\n",
        "# Create node features\n",
        "node_features = torch.tensor(scaled_features, dtype=torch.float)\n",
        "\n",
        "# Create labels (for demonstration, using 'Risk Rating' as the target)\n",
        "labels = torch.tensor(df['Risk Rating'].values, dtype=torch.long)\n",
        "\n",
        "# Create a more meaningful edge index (e.g., based on 'Transaction ID')\n",
        "unique_transactions = df['Transaction ID'].unique()\n",
        "edge_index = []\n",
        "for transaction in unique_transactions:\n",
        "    involved_nodes = df[df['Transaction ID'] == transaction].index.tolist()\n",
        "    for i in range(len(involved_nodes)):\n",
        "        for j in range(i + 1, len(involved_nodes)):\n",
        "            edge_index.append([involved_nodes[i], involved_nodes[j]])\n",
        "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Create train, validation, and test masks\n",
        "train_mask, test_mask = train_test_split(np.arange(df.shape[0]), test_size=0.2, random_state=42)\n",
        "train_mask, val_mask = train_test_split(train_mask, test_size=0.1, random_state=42)\n",
        "\n",
        "train_mask = torch.tensor(train_mask, dtype=torch.long)\n",
        "val_mask = torch.tensor(val_mask, dtype=torch.long)\n",
        "test_mask = torch.tensor(test_mask, dtype=torch.long)\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(x=node_features, edge_index=edge_index, y=labels)\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "class GATv2Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads=8):\n",
        "        super(GATv2Net, self).__init__()\n",
        "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=num_heads, dropout=0.6)\n",
        "        self.conv2 = GATv2Conv(hidden_channels * num_heads, out_channels, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = GATv2Net(in_channels=node_features.shape[1], hidden_channels=32, out_channels=labels.max().item() + 1, num_heads=8)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training and validation function\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    logits, accs = model(data), []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.size(0)\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "# Train the model\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
        "\n",
        "print(f'Final Test Accuracy: {test_acc:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
