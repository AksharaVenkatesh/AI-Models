{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Policyholder_ID First_Name Last_Name Date_of_Birth    Address  \\\n",
      "0                1        Bob       Doe    00-01-1900  33 Oak St   \n",
      "1                2       Jane     Brown    00-01-1900  56 Oak St   \n",
      "2                3      Alice  Williams    00-01-1900  73 Elm St   \n",
      "3                4        Bob     Brown    00-01-1900  84 Oak St   \n",
      "4                5      Alice   Johnson    00-01-1900  23 Oak St   \n",
      "\n",
      "           City State    Zip           Phone                       Email  ...  \\\n",
      "0      New York    IL  81757  (911) 519-4115         Bob.Doe@example.com  ...   \n",
      "1   Los Angeles    IL  42285  (670) 416-1363      Jane.Brown@example.com  ...   \n",
      "2       Chicago    PA  60413  (624) 459-7280  Alice.Williams@example.com  ...   \n",
      "3  Philadelphia    TX  17796  (453) 376-6301       Bob.Brown@example.com  ...   \n",
      "4      New York    PA  16751  (571) 370-9500   Alice.Johnson@example.com  ...   \n",
      "\n",
      "   Claim_Type  Claim_Date Claim_Amount Adjuster_ID  Adjuster_Name  Payment_ID  \\\n",
      "0    Accident  27-07-2022      20000.0         1.0       John Doe           1   \n",
      "1         NaN         NaN          NaN         NaN            NaN           2   \n",
      "2         NaN         NaN          NaN         NaN            NaN           3   \n",
      "3         NaN         NaN          NaN         NaN            NaN           4   \n",
      "4    Accident  29-11-2026       5000.0         1.0     Jane Smith           5   \n",
      "\n",
      "   Payment_Date Payment_Amount Payment_Method  Longevity  \n",
      "0    17-05-2026     861.606370    Credit Card  64.349284  \n",
      "1    29-05-2024    1357.574173    Credit Card  72.184321  \n",
      "2    27-03-2028    1473.161368    Credit Card  67.525659  \n",
      "3    09-06-2027    1396.943366  Bank Transfer  69.777221  \n",
      "4    26-01-2028    1506.650119  Bank Transfer  73.426473  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Index(['Policyholder_ID', 'First_Name', 'Last_Name', 'Date_of_Birth',\n",
      "       'Address', 'City', 'State', 'Zip', 'Phone', 'Email', 'Policy_ID',\n",
      "       'Policy_Type', 'Effective_Date', 'Expiration_Date', 'Premium',\n",
      "       'Coverage_Amount', 'Claim_ID', 'Claim_Type', 'Claim_Date',\n",
      "       'Claim_Amount', 'Adjuster_ID', 'Adjuster_Name', 'Payment_ID',\n",
      "       'Payment_Date', 'Payment_Amount', 'Payment_Method', 'Longevity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the insurance data\n",
    "insurance_data = pd.read_csv(\"C:\\\\Users\\\\AksharaVenkatesh\\\\OneDrive - ConceptVines\\\\High Peak\\\\insurance_data_optim.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(insurance_data.head())\n",
    "# Display the columns in the dataset\n",
    "print(insurance_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Policyholder_ID', 'First_Name', 'Last_Name', 'Date_of_Birth',\n",
      "       'Address', 'City', 'State', 'Zip', 'Phone', 'Email', 'Policy_ID',\n",
      "       'Policy_Type', 'Effective_Date', 'Expiration_Date', 'Premium',\n",
      "       'Coverage_Amount', 'Claim_ID', 'Claim_Type', 'Claim_Date',\n",
      "       'Claim_Amount', 'Adjuster_ID', 'Adjuster_Name', 'Payment_ID',\n",
      "       'Payment_Date', 'Payment_Amount', 'Payment_Method', 'Longevity'],\n",
      "      dtype='object')\n",
      "Number of nodes: 601\n",
      "Number of edges: 1000\n",
      "insurance_graph.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"insurance_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1e9a32870e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from torch_geometric.utils import from_networkx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load the insurance data\n",
    "insurance_data = pd.read_csv('C:\\\\Users\\\\AksharaVenkatesh\\\\OneDrive - ConceptVines\\\\High Peak\\\\insurance_data_optim.csv')\n",
    "\n",
    "# Initialize a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Display the columns in the dataset to verify the available columns\n",
    "print(insurance_data.columns)\n",
    "\n",
    "# Extract relevant columns\n",
    "policyholders = insurance_data[['Policyholder_ID', 'First_Name', 'Last_Name', 'Date_of_Birth', 'Address', 'City', 'State', 'Zip', 'Phone', 'Email']].drop_duplicates()\n",
    "policies = insurance_data[['Policy_ID', 'Policyholder_ID', 'Policy_Type', 'Effective_Date', 'Expiration_Date', 'Premium', 'Coverage_Amount']].drop_duplicates()\n",
    "claims = insurance_data[['Claim_ID', 'Policy_ID', 'Claim_Date', 'Claim_Type', 'Claim_Amount', 'Adjuster_ID']].drop_duplicates()\n",
    "\n",
    "# Add policyholder nodes\n",
    "for _, row in policyholders.iterrows():\n",
    "    G.add_node(str(row['Policyholder_ID']), type='policyholder', features=row.to_dict())\n",
    "\n",
    "# Add policy nodes and edges to policyholders\n",
    "for _, row in policies.iterrows():\n",
    "    G.add_node(str(row['Policy_ID']), type='policy', features=row.to_dict())\n",
    "    G.add_edge(str(row['Policyholder_ID']), str(row['Policy_ID']), relation='holds')\n",
    "\n",
    "# Add claim nodes and edges to policies\n",
    "for _, row in claims.iterrows():\n",
    "    G.add_node(str(row['Claim_ID']), type='claim', features=row.to_dict())\n",
    "    G.add_edge(str(row['Policy_ID']), str(row['Claim_ID']), relation='files')\n",
    "\n",
    "# Calculate age from Date_of_Birth and add it to policyholder features\n",
    "def calculate_age(birthdate):\n",
    "    today = datetime.today()\n",
    "    return today.year - birthdate.year - ((today.month, today.day) < (birthdate.month, birthdate.day))\n",
    "\n",
    "# Function to parse the date safely\n",
    "def safe_parse_date(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d-%m-%Y')\n",
    "    except ValueError:\n",
    "        return None  # Return None if date is invalid\n",
    "\n",
    "for _, row in policyholders.iterrows():\n",
    "    birthdate = safe_parse_date(row['Date_of_Birth'])\n",
    "    if birthdate is not None:\n",
    "        age = calculate_age(birthdate)\n",
    "        G.nodes[str(row['Policyholder_ID'])]['features']['age'] = age\n",
    "    else:\n",
    "        G.nodes[str(row['Policyholder_ID'])]['features']['age'] = None  # Handle invalid date\n",
    "\n",
    "# Verify the graph construction\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Convert the NetworkX graph to a PyTorch Geometric Data object\n",
    "data = from_networkx(G)\n",
    "\n",
    "# Use the original NetworkX graph directly for visualization\n",
    "G_nx = G\n",
    "\n",
    "# Create a Pyvis network\n",
    "net = Network(notebook=True, height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", cdn_resources='remote')\n",
    "\n",
    "# Add nodes and edges to the pyvis network\n",
    "for node, node_attrs in G_nx.nodes(data=True):\n",
    "    net.add_node(node, label=str(node), title=str(node_attrs['features']), **node_attrs)\n",
    "\n",
    "for source, target, edge_attrs in G_nx.edges(data=True):\n",
    "    net.add_edge(source, target, title=str(edge_attrs))\n",
    "\n",
    "# Generate the HTML file\n",
    "net.show(\"insurance_graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Policyholder_ID First_Name Last_Name Date_of_Birth    Address  \\\n",
      "0                1        Bob       Doe    00-01-1900  33 Oak St   \n",
      "1                2       Jane     Brown    00-01-1900  56 Oak St   \n",
      "2                3      Alice  Williams    00-01-1900  73 Elm St   \n",
      "3                4        Bob     Brown    00-01-1900  84 Oak St   \n",
      "4                5      Alice   Johnson    00-01-1900  23 Oak St   \n",
      "\n",
      "           City State    Zip           Phone                       Email  ...  \\\n",
      "0      New York    IL  81757  (911) 519-4115         Bob.Doe@example.com  ...   \n",
      "1   Los Angeles    IL  42285  (670) 416-1363      Jane.Brown@example.com  ...   \n",
      "2       Chicago    PA  60413  (624) 459-7280  Alice.Williams@example.com  ...   \n",
      "3  Philadelphia    TX  17796  (453) 376-6301       Bob.Brown@example.com  ...   \n",
      "4      New York    PA  16751  (571) 370-9500   Alice.Johnson@example.com  ...   \n",
      "\n",
      "   Claim_Type  Claim_Date Claim_Amount Adjuster_ID  Adjuster_Name  Payment_ID  \\\n",
      "0    Accident  27-07-2022      20000.0         1.0       John Doe           1   \n",
      "1         NaN         NaN          NaN         NaN            NaN           2   \n",
      "2         NaN         NaN          NaN         NaN            NaN           3   \n",
      "3         NaN         NaN          NaN         NaN            NaN           4   \n",
      "4    Accident  29-11-2026       5000.0         1.0     Jane Smith           5   \n",
      "\n",
      "   Payment_Date Payment_Amount Payment_Method  Longevity  \n",
      "0    17-05-2026     861.606370    Credit Card  64.349284  \n",
      "1    29-05-2024    1357.574173    Credit Card  72.184321  \n",
      "2    27-03-2028    1473.161368    Credit Card  67.525659  \n",
      "3    09-06-2027    1396.943366  Bank Transfer  69.777221  \n",
      "4    26-01-2028    1506.650119  Bank Transfer  73.426473  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the insurance data\n",
    "insurance_data = pd.read_csv('C:\\\\Users\\\\AksharaVenkatesh\\\\OneDrive - ConceptVines\\\\High Peak\\\\insurance_data_optim.csv')\n",
    "print(insurance_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Policyholder_ID', 'First_Name', 'Last_Name', 'Date_of_Birth',\n",
      "       'Address', 'City', 'State', 'Zip', 'Phone', 'Email', 'Policy_ID',\n",
      "       'Policy_Type', 'Effective_Date', 'Expiration_Date', 'Premium',\n",
      "       'Coverage_Amount', 'Claim_ID', 'Claim_Type', 'Claim_Date',\n",
      "       'Claim_Amount', 'Adjuster_ID', 'Adjuster_Name', 'Payment_ID',\n",
      "       'Payment_Date', 'Payment_Amount', 'Payment_Method', 'Longevity'],\n",
      "      dtype='object')\n",
      "Number of nodes: 1300\n",
      "Number of edges: 1399\n",
      "Node features shape: torch.Size([1300, 6])\n",
      "Adjacency matrix shape: torch.Size([1300, 1300])\n",
      "Number of training samples: 1051\n",
      "Number of test samples: 249\n",
      "Epoch 1/50, Loss: 0.0\n",
      "Epoch 2/50, Loss: 0.0\n",
      "Epoch 3/50, Loss: 0.0\n",
      "Epoch 4/50, Loss: 0.0\n",
      "Epoch 5/50, Loss: 0.0\n",
      "Epoch 6/50, Loss: 0.0\n",
      "Epoch 7/50, Loss: 0.0\n",
      "Epoch 8/50, Loss: 0.0\n",
      "Epoch 9/50, Loss: 0.0\n",
      "Epoch 10/50, Loss: 0.0\n",
      "Epoch 11/50, Loss: 0.0\n",
      "Epoch 12/50, Loss: 0.0\n",
      "Epoch 13/50, Loss: 0.0\n",
      "Epoch 14/50, Loss: 0.0\n",
      "Epoch 15/50, Loss: 0.0\n",
      "Epoch 16/50, Loss: 0.0\n",
      "Epoch 17/50, Loss: 0.0\n",
      "Epoch 18/50, Loss: 0.0\n",
      "Epoch 19/50, Loss: 0.0\n",
      "Epoch 20/50, Loss: 0.0\n",
      "Epoch 21/50, Loss: 0.0\n",
      "Epoch 22/50, Loss: 0.0\n",
      "Epoch 23/50, Loss: 0.0\n",
      "Epoch 24/50, Loss: 0.0\n",
      "Epoch 25/50, Loss: 0.0\n",
      "Epoch 26/50, Loss: 0.0\n",
      "Epoch 27/50, Loss: 0.0\n",
      "Epoch 28/50, Loss: 0.0\n",
      "Epoch 29/50, Loss: 0.0\n",
      "Epoch 30/50, Loss: 0.0\n",
      "Epoch 31/50, Loss: 0.0\n",
      "Epoch 32/50, Loss: 0.0\n",
      "Epoch 33/50, Loss: 0.0\n",
      "Epoch 34/50, Loss: 0.0\n",
      "Epoch 35/50, Loss: 0.0\n",
      "Epoch 36/50, Loss: 0.0\n",
      "Epoch 37/50, Loss: 0.0\n",
      "Epoch 38/50, Loss: 0.0\n",
      "Epoch 39/50, Loss: 0.0\n",
      "Epoch 40/50, Loss: 0.0\n",
      "Epoch 41/50, Loss: 0.0\n",
      "Epoch 42/50, Loss: 0.0\n",
      "Epoch 43/50, Loss: 0.0\n",
      "Epoch 44/50, Loss: 0.0\n",
      "Epoch 45/50, Loss: 0.0\n",
      "Epoch 46/50, Loss: 0.0\n",
      "Epoch 47/50, Loss: 0.0\n",
      "Epoch 48/50, Loss: 0.0\n",
      "Epoch 49/50, Loss: 0.0\n",
      "Epoch 50/50, Loss: 0.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# Load the insurance data\n",
    "insurance_data = pd.read_csv('insurance_data_optim.csv')\n",
    "print(insurance_data.columns)\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocess_policyholders(data):\n",
    "    columns = ['Policyholder_ID', 'First_Name', 'Last_Name', 'Date_of_Birth', 'City', 'State', 'Zip', 'Phone', 'Email']\n",
    "    available_columns = [col for col in columns if col in data.columns]\n",
    "    policyholders = data[available_columns].drop_duplicates()\n",
    "    return policyholders\n",
    "\n",
    "def preprocess_policies(data):\n",
    "    columns = ['Policy_ID', 'Policyholder_ID', 'Policy_Type', 'Effective_Date', 'Expiration_Date', 'Premium', 'Coverage_Amount', 'Deductible']\n",
    "    available_columns = [col for col in columns if col in data.columns]\n",
    "    policies = data[available_columns].drop_duplicates()\n",
    "    return policies\n",
    "\n",
    "def preprocess_claims(data):\n",
    "    columns = ['Claim_ID', 'Policy_ID', 'Claim_Date', 'Claim_Type', 'Claim_Amount', 'Adjuster_ID']\n",
    "    available_columns = [col for col in columns if col in data.columns]\n",
    "    claims = data[available_columns].drop_duplicates()\n",
    "    return claims\n",
    "\n",
    "def preprocess_payments(data):\n",
    "    columns = ['Payment_ID', 'Policy_ID', 'Payment_Date', 'Payment_Amount', 'Payment_Method']\n",
    "    available_columns = [col for col in columns if col in data.columns]\n",
    "    payments = data[available_columns].drop_duplicates()\n",
    "    return payments\n",
    "\n",
    "# Preprocess the data\n",
    "policyholders = preprocess_policyholders(insurance_data)\n",
    "policies = preprocess_policies(insurance_data)\n",
    "claims = preprocess_claims(insurance_data)\n",
    "payments = preprocess_payments(insurance_data)\n",
    "\n",
    "# Construct the graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add policyholders nodes\n",
    "for index, row in policyholders.iterrows():\n",
    "    G.add_node(row['Policyholder_ID'], type='policyholder', features=row.to_dict())\n",
    "\n",
    "# Add policies nodes and edges\n",
    "for index, row in policies.iterrows():\n",
    "    G.add_node(row['Policy_ID'], type='policy', features=row.to_dict())\n",
    "    G.add_edge(row['Policyholder_ID'], row['Policy_ID'], relation='holds')\n",
    "\n",
    "# Add claims nodes and edges\n",
    "for index, row in claims.iterrows():\n",
    "    G.add_node(row['Claim_ID'], type='claim', features=row.to_dict())\n",
    "    G.add_edge(row['Policy_ID'], row['Claim_ID'], relation='files')\n",
    "    G.add_edge(row['Claim_ID'], row['Adjuster_ID'], relation='assigned')\n",
    "\n",
    "# Add payments nodes and edges\n",
    "for index, row in payments.iterrows():\n",
    "    G.add_node(row['Payment_ID'], type='payment', features=row.to_dict())\n",
    "    G.add_edge(row['Policy_ID'], row['Payment_ID'], relation='pays')\n",
    "\n",
    "# Normalize numeric features (example, depending on your feature structure)\n",
    "def normalize_features(features):\n",
    "    numeric_features = {k: v for k, v in features.items() if isinstance(v, (int, float))}\n",
    "    if numeric_features:\n",
    "        numeric_features_series = pd.Series(numeric_features)\n",
    "        normalized_features = {k: (v - numeric_features_series.mean()) / (numeric_features_series.std() + 1e-6) for k, v in numeric_features.items()} # Added small constant\n",
    "        features.update(normalized_features)\n",
    "    return features\n",
    "\n",
    "for node in G.nodes():\n",
    "    if 'features' in G.nodes[node]:\n",
    "        G.nodes[node]['features'] = normalize_features(G.nodes[node]['features'])\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Reindex nodes to ensure indices are within a manageable range\n",
    "mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "# Extract numeric node features and adjacency matrix\n",
    "node_features = []\n",
    "node_labels = []\n",
    "for node, data in G.nodes(data=True):\n",
    "    if 'features' in data:\n",
    "        feature_vector = [v for v in data['features'].values() if isinstance(v, (int, float))]\n",
    "        node_features.append(feature_vector)\n",
    "        if 'Policy_Type' in data['features']:\n",
    "            node_labels.append(data['features']['Policy_Type'])\n",
    "        else:\n",
    "            node_labels.append('')\n",
    "    else:\n",
    "        node_features.append([0.0] * 5)  # Use a default value or an appropriate vector size\n",
    "        node_labels.append('')\n",
    "\n",
    "# Pad feature vectors to ensure consistent length\n",
    "max_length = max(len(f) for f in node_features)\n",
    "node_features = [f + [0.0] * (max_length - len(f)) for f in node_features]\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "node_features = pd.DataFrame(node_features).fillna(0).replace([float('inf'), float('-inf')], 0).values\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "node_labels = torch.tensor(label_encoder.fit_transform(node_labels), dtype=torch.long)\n",
    "\n",
    "# Create sparse adjacency matrix\n",
    "edges = list(G.edges())\n",
    "edge_index = torch.tensor(edges, dtype=torch.int32).t().contiguous()\n",
    "adj_matrix = torch.sparse_coo_tensor(edge_index, torch.ones(len(edges)), (len(G.nodes()), len(G.nodes()))).to_dense()\n",
    "\n",
    "# Example train/test split\n",
    "train_mask = torch.rand(len(node_labels)) < 0.8\n",
    "test_mask = ~train_mask\n",
    "\n",
    "print(f\"Node features shape: {node_features.shape}\")\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n",
    "print(f\"Number of training samples: {train_mask.sum().item()}\")\n",
    "print(f\"Number of test samples: {test_mask.sum().item()}\")\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.matmul(h, self.W)  # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0]  # number of nodes\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "\n",
    "        e = self.leakyrelu(torch.matmul(all_combinations_matrix, self.a).squeeze(1))\n",
    "        return e.view(N, N)\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, n_feat, n_hid, n_class, dropout, alpha, n_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(n_feat, n_hid, dropout=dropout, alpha=alpha, concat=True) for _ in range(n_heads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(n_hid * n_heads, n_class, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.out_att(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Set hyperparameters\n",
    "n_feat = node_features.shape[1]\n",
    "n_hid = 8\n",
    "n_class = len(label_encoder.classes_)\n",
    "dropout = 0.6\n",
    "alpha = 0.2\n",
    "n_heads = 8\n",
    "lr = 0.001 # Reduced learning rate\n",
    "weight_decay = 5e-4\n",
    "epochs = 50\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = GAT(n_feat, n_hid, n_class, dropout, alpha, n_heads)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(node_features, adj_matrix)\n",
    "    loss = loss_fn(output[train_mask], node_labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(node_features, adj_matrix)\n",
    "    preds = output[test_mask].max(1)[1].type_as(node_labels)\n",
    "    accuracy = preds.eq(node_labels[test_mask]).double().mean()\n",
    "    print(f\"Test accuracy: {accuracy.item()}\")\n",
    "\n",
    "# Generate recommendations\n",
    "def generate_recommendations(model, features, adj, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features, adj)\n",
    "        recommendations = output[mask].max(1)[1].type_as(node_labels)\n",
    "        return recommendations\n",
    "\n",
    "# Generate recommendations for all nodes\n",
    "recommendations = generate_recommendations(model, node_features, adj_matrix, torch.ones_like(train_mask, dtype=torch.bool))\n",
    "\n",
    "# Print recommendations for each policyholder\n",
    "for node, recommendation in zip(G.nodes(), recommendations):\n",
    "    if G.nodes[node].get('type') == 'policyholder':\n",
    "        print(f\"Policyholder {node}: Recommended policy type {label_encoder.inverse_transform([recommendation.item()])[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
